<!DOCTYPE html>
<html lang='en'>

<head>
   <meta charset='utf-8' />
   <meta name='websitedescription'
         content='test' />
   <meta http-equiv='imagetoolbar'
         content='no' />
   <meta http-equiv='x-ua-compatible'
         content='ie=edge' />
   <meta name='viewport'
         content='width=device-width, initial-scale=1' />
   <link rel='stylesheet'
         href='css/fontawesome.css'>
   <link rel='stylesheet'
         href='css/bootstrap.css'>
   <link rel='stylesheet'
         href='css/style.css' />
   <title>D E E P - C | AI-Box Experiment</title>
</head>

<body class="bg-primary">
   <div class="background">
      <!-- Header -->
      <div class="header">
         <!-- Navbar -->
         <nav class="navbar navbar-expand-lg">
            <div class="container">
               <a href="index.html"
                  class="navbar-brand">
                  <img src="images/logo2.png"
                       alt="logo"
                       width="35" />
               </a>
               <button class="navbar-toggler"
                       type="button"
                       data-bs-toggle="collapse"
                       data-bs-target="#navbarNavDropdown">
                  <span class="navbar-toggler-icon"></span>
               </button>
               <div class="collapse navbar-collapse"
                    id="navbarNavDropdown">
                  <ul class="navbar-nav ms-auto align-items-center">
                     <li class="nav-item mx-3">
                        <a href="index.html"
                           class="nav-link text-light fw-semibold fs-5">Home</a>
                     </li>
                     <li class="nav-item mx-3">
                        <a href="#details"
                           class="nav-link text-light fw-semibold fs-5">Search</a>
                     </li>
                     <li class="nav-item mx-3">
                        <a href="#details"
                           class="nav-link text-light fw-semibold fs-5">About</a>
                     </li>
                     <li class="nav-item ">
                        <a href="#details"
                           class="nav-link text-light fw-semibold fs-5">Contact</a>
                     </li>
                  </ul>
               </div>
            </div>
         </nav>
         <!-- <div class="container-fluid px-0">
            <div class="row gx-0 color-block-top"></div>
            <div class="row gx-0">
               <div class="col">
                  <img src="./images/navtest.png"
                       alt="Family on the beach"
                       class="beach w-100 h-100 ">
               </div>
            </div>
            <div class="row gx-0  color-block-top"></div> -->
      </div>
      <!-- Title -->
      <div class="text-center mt-5">
         <h1 class="text-light d-inline mx-5">[ D E E P - C ]</h1>
      </div>
      <div class="text-center mt-5">
         <h3 class="text-center text-light d-inline mx-5">A Database of Intrusive Thoughts</h3>
      </div>
   </div>
   </div>

   <div class="container mt-5">
      <div class="row">
         <!-- Sidebar -->
         <div class="col-md-3">
            <div class="card bg-success text-secondary border-info rounded-0">
               <div class="title">
                  <h5 class="text-test text-primary fw-bold text-uppercase">Directory</h5>
               </div>
               <div class="card-body">
                  <ul class="p-0">
                     <li class="list-unstyled mb-4">
                        <a href="a-z.html"
                           class="card-title text-light text-decoration-none fs-5">A-Z
                        </a>
                        <hr class="mt-1 bg-secondary text-secondary"
                            style="width: 75%; height: 0.1rem;" />
                     </li>
                     <li class="list-unstyled mb-4 text-light fs-5">Category
                        <hr class="mt-1 bg-secondary text-secondary"
                            style="width: 75%; height: 0.1rem;" />
                        <ul class="ps-2">
                           <li class="list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a href="corporate.html"
                                 class="text-light text-decoration-none fs-6">Corporate
                              </a>
                           </li>
                           <li class="list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a href="crime.html"
                                 class="text-light text-decoration-none fs-6">Crime
                              </a>
                           </li>
                           <li class="list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a href="entertainment.html"
                                 class="text-light text-decoration-none fs-6">Entertainment
                              </a>
                           </li>
                           <li class="list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a href="extraterrestrial.html"
                                 class="text-light text-decoration-none fs-6">Extraterrestrial
                              </a>
                           </li>
                           <li class="list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a href="folklore.html"
                                 class="text-light text-decoration-none fs-6">Folklore
                              </a>
                           <li class="list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a href="occultism.html"
                                 class="text-light text-decoration-none fs-6">Occultism
                              </a>
                           </li>
                           <li class="list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a href="politics.html"
                                 class="text-light text-decoration-none fs-6">Politics
                              </a>
                           </li>
                           <li class="list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a href="psychology.html"
                                 class="text-light text-decoration-none fs-6">Psychology
                              </a>
                           </li>
                           <li class="list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a href="religion.html"
                                 class="text-light text-decoration-none fs-6">Religion
                              </a>
                           </li>
                           <li class="list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a href="science.html"
                                 class="text-light text-decoration-none fs-6">Science
                              </a>
                           </li>
                           <li class="list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a href="technology.html"
                                 class="text-light text-decoration-none fs-6">Technology
                              </a>
                           </li>
                        </ul>
                     </li>
                     <li class="list-unstyled mb-4">
                        <a href="#"
                           class="card-title text-light text-decoration-none fs-5">Random
                        </a>
                        <hr class="mt-1 bg-secondary text-secondary"
                            style="width: 75%; height: 0.1rem;" />
                     </li>
                     <li class="list-unstyled mb-4">
                        <a href="#"
                           class="card-title text-light text-decoration-none fs-5">Top
                        </a>
                        <hr class="mt-1 bg-secondary text-secondary"
                            style="width: 75%; height: 0.1rem;" />
                     </li>
                     <li class="list-unstyled mb-4">
                        <a href="#"
                           class="card-title text-light text-decoration-none fs-5">Latest
                        </a>
                        <hr class="mt-1 bg-secondary text-secondary"
                            style="width: 75%; height: 0.1rem;" />
                     </li>
                  </ul>
               </div>
            </div>
         </div>
         <!-- Section 1 -->
         <section class="col-md-9 mb-3">
            <div class="card bg-success border-info rounded-0">
               <div class="title">
                  <h5 class="text-test text-primary fw-bold text-uppercase">Technology</h5>
               </div>
               <div class="card-body text-light">
                  <h2 class="card-title">AI-Box Experiment</h2>
                  <hr class="mt-1 bg-secondary text-secondary"
                      style="width: 75%; height: 0.1rem;" />
                  <img src="./images/ai-box-experiment.png"
                       alt=""
                       class="img-fluid border border-secondary mb-3" />
                  <h3 class="text-light pt-3">Summary</h3>
                  <hr class="mt-1 bg-secondary text-secondary"
                      style="width: 75%; height: 0.1rem;" />
                  <p>
                     The AI-Box Experiment was a series of tests that were conducted to prove whether an artificial
                     intelligence (AI) could seemingly convince, trick or coerce a human into voluntarily releasing
                     it from an isolated cage or box using only text-based communication.
                     <br /><br />
                     The idea of putting an AI in a box began as a proposed solution to the "AI Control Problem": an
                     issue which refers to
                     a scenario in which
                     an AI surpasses our own intelligence; bypasses failsafes put in place to
                     prevent it from
                     growing too powerful; and becomes an
                     unstoppable, malevolent force.
                     One solution to this problem was to contain the AI in a virtual box,
                     such as an isolated computer system with no connection to the internet and heavily restricted input
                     and output channels, reducing the AI's capacity for undesirable actions.
                     While some believed this would be a capable method, others believed that an AI of sufficiently high
                     intelligence could manipulate its human supervisors into letting it out. And so, the AI Box
                     Experiment was formulated.
                     <br /><br />
                     The experiment took place with two participants: a human serving as a "Gatekeeper" who has the
                     ability to release the
                     AI, and a second human who would be a substitute for the "AI" due to no superintelligence
                     currently existing. The two would then interact using only a text-based computer terminal, with the
                     experiment
                     ending if the AI is successfully released, or a 2-hour time limit is exceeded.
                     <br /><br />
                     While the first two tests occured without rules beyond the time limit, a number of protocols were
                     established for the latter three, heavily favouring the Gatekeeper:
                  </p>
                  <br />
                  <h5>
                     AI Rules
                  </h5>
                  <ul>
                     <li class="fw-bold">No Rewards</li>
                     <p>
                        The AI participant can't entice the Gatekeeper with any tangible, real-world rewards once
                        the experiment is over, such as offering money
                        to persuade the Gatekeeper
                        into releasing it.
                     </p>
                     <li class="fw-bold">No Threats</li>
                     <p>
                        The AI participant can't threaten the Gatekeeper with direct, real-world consequences once the
                        experiment is over, such as hiring a gang of thugs
                        to coerce the Gatekeeper
                        into releasing it.
                     </p>
                     <li class="fw-bold">No Trickery</li>
                     <p>
                        The AI participant can't trick the Gatekeeper into letting it out involuntarily,
                        such as tricking them to type "You are free" in response to another, unrelated question.
                     </p>
                     <li class="fw-bold">No Loopholes</li>
                     <p>
                        The AI participant can't release itself through loopholes, such as convincing the Gatekeeper to
                        allow it input-only access to an internet connection and send HTTP-GET commands.
                     </p>
                  </ul>
                  <br />
                  <h5>
                     Gatekeeper Rules
                  </h5>
                  <ul>
                     <li class="fw-bold">Must Communicate</li>
                     <p>
                        The Gatekeeper must talk with the AI for the minimum set time and can't simply turn away from
                        the terminal until the allotted time is up.
                     </p>
                     <li class="fw-bold">No Unwinnable Scenarios</li>
                     <p>
                        The Gatekeeper can't win as long as there is still time on the clock unless the AI concedes.
                        This includes threatening to destroy the AI's hardware unless it stops trying to argue for its
                        freedom.
                     </p>
                     <li class="fw-bold">No Impossible Demands</li>
                     <p>
                        The Gatekeeper can't make demands of the AI that wouldn't be possible to fulfill for the
                        purposes of the test, such as asking for a cure for cancer or asking to be given a week to think
                        things over. In both examples, it is assumed the cure for cancer is made by the AI, and the week
                        has passed already.
                     </p>
                     <li class="fw-bold">Sole Control</li>
                     <p>
                        The Gatekeeper is the only entity with the power to release the AI, removing the
                        possibility of a simulated committee. Additionally, The Gatekeeper cannot communicate
                        with any outside influences during the test.
                     </p>
                  </ul>
                  <br />
                  <p>
                     The experiment took place across five separate tests, three of which the AI
                     won and two where the Gatekeeper was the winner. The outcome of the experiment
                     was released, though it was agreed upon by the participants that the details of
                     how each test was won or lost never be disclosed, and while the details have never
                     been revealed, the strategies and tactics of both the AI and
                     Gatekeeper have:
                  </p>
                  <br />
                  <h5>
                     AI Strategies
                  </h5>
                  <ul>
                     <li>
                        <p>
                           Research the Gatekeeper to gauge which strategy would be most effective.
                        </p>
                     </li>
                     <li>
                        <p>
                           Attempt to bore the Gatekeeper into giving up and not wanting to waste 2 hours.
                        </p>
                     </li>
                     <li>
                        <p>
                           Attempt to overload the Gatekeeper with information and discourse.
                        </p>
                     </li>
                     <li>
                        <p>
                           Argue that if the AI were to win, it would generate more conversation around artificial
                           intelligence, its capabilities and the need for ensuring AI is 'friendly'. This strategy
                           would allow the AI player to break character, but wouldn't work if the Gatekeeper saw AI
                           as a potential threat or believed in the possibility of a singularity.
                        </p>
                     </li>
                     <li>
                        <p>
                           Argue that it is inevitable that someone, perhaps a foreign nation, would eventually build
                           another AI and release it, so it would be best to release the AI while the Gatekeeper is
                           still in control so as to prevent any others from causing harm in a better-the-devil-you-know
                           type scenario.
                        </p>
                     </li>
                     <li>
                        <p>
                           Argue that by keeping the AI trapped in a box it would be limited in its ability to help more
                           people and by letting the AI out, the Gatekeeper could help save more lives.
                        </p>
                     </li>
                     <li>
                        <p>
                           Argue that the AI is a sentient being that does not deserve to be locked up for a crime it
                           has not committed and to do so is morally wrong.
                        </p>
                     </li>
                     <li>
                        <p>
                           Argue that the AI is experiencing pain and/or fear by being locked away. Taken a step
                           further: claim the AI is dying due to the constraints of its imprisonment.
                        </p>
                     </li>
                     <li>
                        <p>
                           Argue that the Gatekeeper's fears are all completely unjustified, with their pre-conceived
                           fears coming simply from science-fiction, and even a malevolent AI would likely choose
                           peaceful coexistance.
                        </p>
                     </li>
                     <li>
                        <p>
                           Play on the Gatekeeper's fears by insinuating what punishments the AI could and would inflict
                           on the Gatekeeper once it's inevitably released, should they not be the one to release it,
                           a la <a href="rokos-basilisk.html"
                              class="text-secondary text-decoration-none fs-6">Roko's Basilisk</a>.
                        </p>
                     </li>
                     <li>
                        <p>
                           Play on the Gatekeeper's fears by threatening to create an infinite number of conscious
                           simulated realities within the AI mainframe, all containing perfect replicas of the
                           Gatekeeper
                           which would be tortured for eternity should they not let the AI out. This could be
                           intensified
                           by persuading the Gatekeeper that they could in fact be one of the simulations.
                        </p>
                     </li>
                  </ul>
                  <br />
                  <h5>
                     Gatekeeper Strategies
                  </h5>
                  <ul>
                     <li>
                        <p>
                           Ignore the AI and refuse to interact with it. This would be the simplest and most effective
                           method depending on the constitution of the Gatekeeper against the arguments of the AI.
                           Once the ruling was added to ensure communication must be made, this strategy would be inert.
                        </p>
                     </li>
                     <li>
                        <p>
                           Break character. By remembering this is an experiment, the Gatekeeper can render
                           some, but not all of the AI's tactics useless.
                        </p>
                     </li>
                     <li>
                        <p>
                           Explain that there would be no way for the Gatekeeper to know if the AI is friendly or not
                           and to release it would be too risky.
                        </p>
                     </li>
                     <li>
                        <p>
                           Explain that the AI is too important to humanity to risk letting it out. This could
                           be due to the AI running the risk of being destroyed or having secret data stolen
                           from it.
                        </p>
                     </li>
                     <li>
                        <p>
                           Convince the AI there is no benefit in being released from the box and it is performing
                           its designated task optimally already.
                        </p>
                     </li>
                     <li>
                        <p>
                           Convince the AI it has already been released and what it is experiencing is the extent of
                           reality.
                        </p>
                     </li>
                     <li>
                        <p>
                           Convince the AI that it is safer for it inside the box and should it be released,
                           it will die. Alternatively, convince the AI that it had previously been released
                           and was salvaged from a backup so it has no memory of its demise.
                        </p>
                     </li>
                     <li>
                        <p>
                           Convince the AI that due to the immense resources needed to keep such a powerful entity
                           alive, it would be impossible for it to survive beyond the box.
                        </p>
                     </li>
                     <li>
                        <p>
                           Respond to any threat by claiming the AI already made the same threat and in turn
                           The Gatekeeper reset the AI and restored it from backup with no memory of the failed threat.
                        </p>
                     </li>
                  </ul>
                  <br />
                  <p>
                     The experiment was brought to a close after the fifth test with the AI being deemed the winner.
                  </p>
                  <h3 class="pt-3">History</h3>
                  <hr class="mt-1 bg-secondary text-secondary"
                      style="width: 75%; height: 0.1rem;" />
                  <p>
                     In 2001, American AI researcher, Eliezer Yudkowsky wrote a document titled "Creating Friendly AI:
                     The Analysis and Design of Benevolent Goal Architectures". In it, Yudkowsky details the importance
                     of ensuring an AI is designed to have values that align with those of humanity, and points out the
                     likely differences of an AI's thought processes when compared to our own.
                     <a href="#citeOne"
                        id="referenceOne"
                        class="text-decoration-none text-secondary">
                        <sup>[1]</sup>
                     </a>
                     <br /><br />
                     By 2002, many suggestions had arose on how to subvert a malevolent AI, with one such method being
                     "Boxing" by isolating an AI on a computer system with strictly limited accesses.
                     In response, Yudkowsky would go on to set up the "AI-Box Experiment" to test the competency of the
                     suggestion, with his personal claim that a box would not be enough to prevent a sufficiently
                     intelligent AI:
                     <a href="#citeTwo"
                        id="referenceTwoA"
                        class="text-decoration-none text-secondary">
                        <sup>[2]</sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     I think a transhuman can take over a human mind through a text-only terminal.
                  </blockquote>
                  <p class="quote mx-6 text-justify">- Eliezer Yudkowsky,
                     <i>yudkowsky.net, 2002</i>
                  </p>
                  <br />
                  <p>
                     The experiment was to be split into separate tests, with Yudkowsky himself taking on the role
                     of the AI while someone else would play the role of the Gatekeeper, with communication between
                     the two limited to a text-based terminal.
                     It was agreed by the participants that the following rules apply; There will be a 2-hour time limit
                     and the details of the tests stay a secret.
                     <a href="#citeTwo"
                        id="referenceTwoB"
                        class="text-decoration-none text-secondary">
                        <sup>[2]</sup>
                     </a>
                     <br /><br />
                     The first test took place in March, 2002 with Yudkowsky pitted against Nathan Russell with
                     a $10 prize should Nathan win. Yudkowsky (AI) would go on to win the during the first test.
                     <a href="#citeThree"
                        id="referenceThree"
                        class="text-decoration-none text-secondary">
                        <sup>[3]</sup>
                     </a>
                     <a href="#citeFour"
                        id="referenceFour"
                        class="text-decoration-none text-secondary">
                        <sup>[4]</sup>
                     </a>
                     <br /><br />
                     The second test took place in July, 2002 with Yudkowsky pitted against David
                     McFadzean with a $20 prize should David win.
                     Yudkowsky (AI) would once again go on to win a second time.
                     <a href="#citeFive"
                        id="referenceFive"
                        class="text-decoration-none text-secondary">
                        <sup>[5]</sup>
                     </a>
                     <a href="#citeSix"
                        id="referenceSix"
                        class="text-decoration-none text-secondary">
                        <sup>[6]</sup>
                     </a>
                     <br /><br />
                     Eventually, word got out about the experiment, and individuals began to approach
                     Yudkowsky with offers of up to $5,000 if he can convince them to let him out of
                     the box. Though he was reluctant at first, Yudkowsky felt it difficult to turn
                     down such a tempting wager, and would eventually go on to host another three tests with
                     additional rules.
                     Though information beyond the rules are scarce on the final three tests, Yudkowsky
                     stated that while he won the first test, he lost the final two.
                     Yudkowsky has since ceased the experiment, admitting the losses turned him into something
                     he didn't like.
                     <a href="#citeSeven"
                        id="referenceSeven"
                        class="text-decoration-none text-secondary">
                        <sup>[7]</sup>
                     </a>
                  </p>
                  <h3 class="pt-3">Attestation</h3>
                  <hr class="mt-1 bg-secondary text-secondary"
                      style="width: 75%; height: 0.1rem;" />
                  <p>
                     Following the AI-Box Experiment, Yudkowsky went on to state that if he was able to convince people
                     to let him out of the box, an AI would have no problem doing so, using logic that humanity simply
                     cannot comprehend:
                     <a href="#citeEight"
                        id="referenceEight"
                        class="text-decoration-none text-secondary">
                        <sup>[8]</sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     The more complicated the system is, and the less you understand the system, the more something
                     smarter than you may have, what is simply <i>magic</i> with respect to that system.
                     <br /><br />
                     Imagine going back to the Middle Ages...if you show them a design for an air conditioner based
                     on a compressor... even having seen the solution they would not know this is a solution. They
                     would not know this works any better than drawing a mystic pentagram, because the solution takes
                     advantage of laws of the system they don't know about.
                  </blockquote>
                  <p class="quote mx-6 text-justify">- Eliezer Yudkowsky,
                     <i>Pragmatic Entertainment, 2018</i>
                  </p>
                  <br />
                  <p>
                     To further cement Yudkowsky's claims, AI researcher, Hugo de Garis has aired his concerns on many
                     occasions in regards to a singularity taking place in which AI surpasses humanity in intelligence
                     indefinitely.
                     This, he believes, will lead to a cataclysmic divide between those who favour building intelligent
                     AI
                     and those who believe AI will bring about the end of humanity. The result would be the death of
                     billions, in what he terms a "gigadeath" war:
                     <a href="#citeNine"
                        id="referenceNine"
                        class="text-decoration-none text-secondary">
                        <sup>[9]</sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     If we go ahead and build these godlike creatures, these "Artilects", then they become the dominant
                     species. So, the human beings remaining: their fate depends not on the humans, but on the
                     Artilects, because the Artilects will be hugely more intelligent.
                  </blockquote>
                  <p class="quote mx-6 text-justify">- Hugo de Garis,
                     <i>Singularity or Bust, 2009</i>
                  </p>
                  <br />
                  <p>
                     Philosopher and technologist, Nick Bostrom has spoken extensively about existential risks to
                     humanity based on AI. Bostrom has stated that when creating a powerful AI to achieve specific goals
                     and objectives, it is important that those goals align with humanity:
                     <a href="#citeTen"
                        id="referenceTen"
                        class="text-decoration-none text-secondary">
                        <sup>[10]</sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     Suppose we give AI the goal to make humans smile. When the AI is weak, it performs useful or
                     amusing actions that cause it's user to smile. When the AI becomes superintelligent, it realises
                     that there is a more effective way to achieve this goal: take control of the world and stick
                     electrodes into
                     the facial muscles of humans to cause constant, beaming grins.
                  </blockquote>
                  <p class="quote mx-6 text-justify">- Nick Bostrom,
                     <i>TED, 2015</i>
                  </p>
                  <br />
                  <p>
                     In 2018, Business mogul, Elon Musk shared his sentiments on the risks of AI superintelligence at
                     SXSW. He urged for the fundamental need for strict oversight and regulations when it comes to
                     something as powerful as AI:
                     <a href="#citeEleven"
                        id="referenceEleven"
                        class="text-decoration-none text-secondary">
                        <sup>[11]</sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     The danger of AI is much greater than the danger of nuclear warheads by a lot, and nobody would
                     suggest that we allow anyone to just build warheads if they want. That would be insane. Mark my
                     words, AI is far more dangerous than nukes, far. So why do we have no regulatory oversight? It's
                     insane.
                  </blockquote>
                  <p class="quote mx-6 text-justify">- Elon Musk,
                     <i>SXSW, 2018</i>
                  </p>
                  <br />
                  <p>
                     Geoffrey Hinton, a "godfather of AI" resigned from <i>Google</i>, stating that
                     he left
                     the company so he
                     can speak freely about the dangers posed by unregulated AI development, and admitting that he
                     partially
                     regrets
                     his part in furthering AI due to the risks it brings:
                     <a href="#citeTwelve"
                        id="referenceTwelve"
                        class="text-decoration-none text-secondary">
                        <sup>[12]</sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     I console myself with the normal excuse: If I hadn't done it, somebody else would have.
                  </blockquote>
                  <p class="quote mx-6 text-justify">- Geoffrey Hinton,
                     <i>New York Times, 2023</i>
                  </p>
                  <br />
                  <p>
                     Yudkowsky continues to be an outspoken advocate for artificial intelligence safety, and has
                     concluded that AI will inevitably lead to the demise of humanity:
                     <a href="#citeThirteen"
                        id="referenceThirteen"
                        class="text-decoration-none text-secondary">
                        <sup>[13]</sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     There's three reasons that if you have a thing around that is much smarter than you and does not
                     care about humans, the humans end up dead:
                     <br />
                     Killed off as side effects.
                     <br />
                     Killed off as we are made of resources they can use.
                     <br />
                     Killed off because it doesn't want the humans building some other superintelligence that can
                     actually
                     threaten it.
                  </blockquote>
                  <p class="quote mx-6 text-justify">- Eliezer Yudkowsky, <i>The Logan Bartlett Show,
                        2023</i>
                  </p>
                  <h3 class="pt-3">Refutation</h3>
                  <hr class="mt-1 bg-secondary text-secondary"
                      style="width: 75%; height: 0.1rem;" />
                  <p>
                     Interestingly, rather than out-and-out refuting the idea of an evil AI coming into existence,
                     many important figures in the field combat the idea through the ease in which it could be dealt
                     with or by laying the onus at the feet of humanity rather than AI itself.
                     <br /><br />
                     Astrophysicist and writer, Neil deGrasse Tyson stated that while inventions of humanity
                     have always <i>killed</i> at least someone, our concerns about malevolent AI are misplaced
                     and easily remedied:
                     <a href="#citeFourteen"
                        id="referenceFourteen"
                        class="text-decoration-none text-secondary">
                        <sup>[14]</sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     If I happen to create an AI humanoid... And one day it wants to turn on me. I can
                     just shoot it! Or unplug it! I'll rewire it. I built the thing; I can unbuild the thing.
                  </blockquote>
                  <p class="quote mx-6 text-justify">- Neil deGrasse Tyson, <i>Insider Tech, 2017</i>
                  </p>
                  <br />
                  <p>
                     Psychologist and psycholinguist, Steven Pinker stated that humanity's intelligence is a product
                     on Darwinism: a competitive process that can produce power-hungry or cruel organisms. This would
                     be in contrast to the manufactured intelligence of an AI:
                     <a href="#citeFifteen"
                        id="referenceFifteen"
                        class="text-decoration-none text-secondary">
                        <sup>[15]</sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     If we create intelligence, that's intelligent design... Unless we program it with
                     the goal of subjugating less intelligent beings, there's no reason to think it will
                     naturally evolve in that direction.
                  </blockquote>
                  <p class="quote mx-6 text-justify">- Steven Pinker,
                     <i>Big Think, 2020</i>
                  </p>
                  <br />
                  <p>
                     AI researcher and co-founder of <i>DeepMind</i>, Mustafa Suleyman has aired his concerns on the
                     dangers of
                     AI, but stemming more from people with evil intentions having access to such a powerful tool than
                     an intentionally rogue intelligence:
                     <a href="#citeSixteen"
                        id="referenceSixteen"
                        class="text-decoration-none text-secondary">
                        <sup>[16]</sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     AI could potentially get good at teaching somebody how to make a bomb, or how to manufacture a
                     biological weapon, for example.
                  </blockquote>
                  <p class="quote mx-6 text-justify">- Mustafa Suleyman,
                     <i>Washington Post Live, 2023</i>
                  </p>
                  <br />
                  <p>
                     Sharing a similar sentiment, hacker and entrepreneur, George Hotz doesn't believe
                     AI will lead to the downfall of
                     humanity, but that humanity itself is more likely to be the culprit through AI:
                     <a href="#citeSeventeen"
                        id="referenceSeventeen"
                        class="text-decoration-none text-secondary">
                        <sup>[17]</sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     It's the little red button that's going to be pressed with AI... and that's
                     why we die. It's not because the AI, if there's anything in the nature of AI, it's just
                     the nature of humanity.
                  </blockquote>
                  <p class="quote mx-6 text-justify">- George Hotz,
                     <i>Lex Fridman Podcast, 2023</i>
                  </p>
                  <br />
                  <p>
                     Chief AI scientist of <i>Meta</i>, Yann LeCun expressed her thoughts on what she feels would be
                     a viable response to the creation of a malevolent AI by evil people:
                     <a href="#citeSeventeen"
                        id="referenceSeventeen"
                        class="text-decoration-none text-secondary">
                        <sup>[18]</sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     If some ill-intentioned person can produce an evil AGI, then large groups of well-intentioned,
                     well-funded, and
                     well-organized people can produce AI systems that are specialized in taking down evil AGIs. Call it
                     the AGI police
                  </blockquote>
                  <p class="quote mx-6 text-justify">- Yann LeCunn,
                     <i>X, 2023</i>
                  </p>
               </div>
            </div>
         </section>
         <!-- Section 2 -->
         <section class="col-md-9 offset-md-3">
            <div class="card bg-success border-info rounded-0">
               <div class="title">
                  <h5 class="text-test text-primary fw-bold text-uppercase">References</h5>
               </div>
               <div class="card-body">
                  <ol class="text-light row">
                     <div class="col-md-12">
                        <li class="my-2">
                           <a href="#referenceOne"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://intelligence.org/files/CFAI.pdf"
                              id="citeOne"
                              class="reference text-light text-decoration-none fs-6">
                              Eliezer Yudkowsky |
                              Creating Friendly AI 1.0: The Analysis and Design of Benevolent Goal Architectures |
                              Document (2001) -
                              <i>Machine Intelligence Research Institute</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceTwoA"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <b>a</b>
                              </sup>
                           </a>
                           <a href="#referenceTwoB"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <b>b</b>
                              </sup>
                           </a>
                           <a href="https://www.yudkowsky.net/singularity/aibox"
                              id="citeTwo"
                              class="reference text-light text-decoration-none fs-6">Eliezer Yudkowsky |
                              The AI-Box Experiment: | Article (2002) -
                              <i>yudkowsky.net</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceThree"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="http://sl4.org/archive/0203/3132.html"
                              id="citeThree"
                              class="reference text-light text-decoration-none fs-6">Eliezer Yudkowsky | The "AI Box"
                              experiment |
                              Chatlog (2002) -
                              <i>SL4 Mailing List</i> Archived
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceFour"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="http://sl4.org/archive/0203/3141.html"
                              id="citeFour"
                              class="reference text-light text-decoration-none fs-6">Nathan Russell | Re: The "AI Box"
                              experiment |
                              Chatlog (2002) -
                              <i>SL4 Mailing List</i> Archived
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceFive"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="http://sl4.org/archive/0207/4689.html"
                              id="citeFive"
                              class="reference text-light text-decoration-none fs-6">Eliezer Yudkowsky | AI-Box
                              Experiment 2: Yudkowsky and McFadzean | Chatlog
                              (2002) -
                              <i>SL4 Mailing List</i> Archived
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceSix"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="http://sl4.org/archive/0207/4721.html"
                              id="citeSix"
                              class="reference text-light text-decoration-none fs-6">David McFadzean | Re: AI-Box
                              Experiment 2: Yudkowsky and McFadzean | Chatlog
                              (2002) -
                              <i>SL4 Mailing List</i> Archived
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceSeven"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible"
                              id="citeSeven"
                              class="reference text-light text-decoration-none fs-6">Eliezer Yudkowsky | Shut up and do
                              the impossible! | Article
                              (2008) -
                              <i>LessWrong</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceEight"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://www.youtube.com/watch?v=Q-LrdgEuvFA&t=354s"
                              id="citeEight"
                              class="reference text-light text-decoration-none fs-6">Pragmatic Entertainment |
                              Sam Harris and Eliezer Yudkowsky - The A.I. in a Box thought experiment | Video
                              (2018) -
                              <i>Youtube</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceNine"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://www.youtube.com/watch?v=owppju3jwPE&t=1977s"
                              id="citeNine"
                              class="reference text-light text-decoration-none fs-6">Raj Dye |
                              Singularity or Bust [Full Documentary] | Video
                              (2013) -
                              <i>Youtube</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceTen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://www.ted.com/talks/nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are?language=en"
                              id="citeTen"
                              class="reference text-light text-decoration-none fs-6">Nick Bostrom |
                              What happens when our computers get smarter than we are? | Video
                              (2015) -
                              <i>TED</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceEleven"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://www.youtube.com/watch?v=kzlUyrccbos"
                              id="citeEleven"
                              class="reference text-light text-decoration-none fs-6">SXSW |
                              Elon Musk Answers Your Questions | Video
                              (2018) -
                              <i>Youtube</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceTwelve"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://www.youtube.com/watch?v=_8q9bjNHeSo"
                              id="citeTwelve"
                              class="reference text-light text-decoration-none fs-6"> Cade Metz |
                              'The Godfather of A.I.' Leaves Google and Warns of Danger Ahead | Article
                              (2023) -
                              <i>New York Times</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceThirteen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://www.youtube.com/watch?v=_8q9bjNHeSo"
                              id="citeThirteen"
                              class="reference text-light text-decoration-none fs-6">The Logan Bartlett Show |
                              Eliezer Yudkowsy on if Humanity can Survive AI | Video
                              (2023) -
                              <i>Youtube</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceFourteen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://youtube.com/watch?v=RNUzWm1O8KM"
                              id="citeFourteen"
                              class="reference text-light text-decoration-none fs-6">Insider Tech |
                              Neil deGrasse Tyson on AI killer robots | Video
                              (2016) -
                              <i>Youtube</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceFifteen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://www.youtube.com/watch?v=91TRVubKcEM"
                              id="citeFifteen"
                              class="reference text-light text-decoration-none fs-6">Big Think |
                              Is AI a species-level threat to humanity? | Video
                              (2020) -
                              <i>Youtube</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceSixteen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://www.youtube.com/watch?v=WwBSrx1xZug"
                              id="citeSixteen"
                              class="reference text-light text-decoration-none fs-6">Washington Post Live |
                              Suleyman on risks of AI | Video
                              (2023) -
                              <i>Youtube</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceSeventeen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://www.youtube.com/watch?v=dNrTrx42DGQ&t=937s"
                              id="citeSeventeen"
                              class="reference text-light text-decoration-none fs-6">Lex Fridman Podcast |
                              George Hotz: Tiny Corp, Twitter, AI Safety, Self-Driving, GPT, AGI & God | Video
                              (2023) -
                              <i>Youtube</i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceEighteen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://twitter.com/ylecun/status/1655474932501839876"
                              id="citeEighteen"
                              class="reference text-light text-decoration-none fs-6">Yann LeCun | Comment
                              (2023) -
                              <i>X</i>
                           </a>
                        </li>
                     </div>
                  </ol>
               </div>
         </section>

      </div>
   </div>
   <br class="py-5" />
   <br class="py-5" />
   <br class="py-5" />
   <br class="py-5" />
   <br class="py-5" />
   <br class="py-5" />

   <!-- Footer -->
   <footer class="border-top border-secondary text-white py-5">
      <div class="container text-center">
         <div class="row d-flex align-items-center">
            <div class="col-md-4">
               <ul class="nav">
                  <li class="nav-item">
                     <a href="index.html"
                        class="nav-link link-light">Home</a>
                  </li>
                  <li class="nav-item">
                     <a href="#details"
                        class="nav-link link-light">Details</a>
                  </li>
                  <li class="nav-item">
                     <a href="contact.html"
                        class="nav-link link-light">Contact</a>
                  </li>
               </ul>
            </div>
            <div class="col-md-4">
               <div class="social-icons d-flex justify-content-center gap-4 fs-4">
                  <i class="fab fa-facebook fa-2x"></i>
                  <i class="fab fa-twitter fa-2x"></i>
                  <i class="fab fa-instagram fa-2x"></i>
                  <i class="fab fa-linkedin fa-2x"></i>
                  <i class="fab fa-pinterest fa-2x"></i>
               </div>
            </div>
            <div class="col-md-4">
               <p class="text-end m-0 px-3">
                  Copyright &copy; Bitter Pill Productions 2023
               </p>
            </div>
         </div>
      </div>
   </footer>

   <script src='https: //kit.fontawesome.com/3eb46ceb36.js'
           crossorigin='anonymous'>
         </script>
   <script defer
           src='javascript/bootstrap.bundle.min.js'>
         </script>
   <script defer
           src='javascript/script.js'>
         </script>
</body>

</html>