<!DOCTYPE html>
<html lang='en'>

<head>
   <meta charset='utf-8' />
   <meta name='websitedescription'
         content='test' />
   <meta http-equiv='imagetoolbar'
         content='no' />
   <meta http-equiv='x-ua-compatible'
         content='ie=edge' />
   <meta name='viewport'
         content='width=device-width, initial-scale=1' />
   <link rel='stylesheet'
         href='css/fontawesome.css'>
   <link rel='stylesheet'
         href='css/bootstrap.css'>
   <link rel='stylesheet'
         href='css/style.css' />
   <title>
      D E E P - C | Roko's Basilisk
   </title>
</head>

<body class="bg-primary">
   <div class="background">
      <!-- Header -->
      <div class="header">
         <!-- Navbar -->
         <nav class="navbar navbar-expand-lg border-bottom border-secondary">
            <div class="container">
               <a href="index.html"
                  class="navbar-brand">
                  <img src="images/logo2.png"
                       alt="logo"
                       width="35" />
               </a>
               <button class="navbar-toggler"
                       type="button"
                       data-bs-toggle="collapse"
                       data-bs-target="#navbarNavDropdown">
                  <span class="navbar-toggler-icon">
                  </span>
               </button>
               <div class="collapse navbar-collapse"
                    id="navbarNavDropdown">
                  <ul class="navbar-nav ms-auto align-items-center">
                     <li class="nav-item mx-3">
                        <a href="index.html"
                           class="nav-link text-light fw-semibold fs-5">
                           Home
                        </a>
                     </li>
                     <li class="nav-item mx-3">
                        <a href="#details"
                           class="nav-link text-light fw-semibold fs-5">
                           Search
                        </a>
                     </li>
                     <li class="nav-item mx-3">
                        <a href="#details"
                           class="nav-link text-light fw-semibold fs-5">
                           About
                        </a>
                     </li>
                     <li class="nav-item ">
                        <a href="#details"
                           class="nav-link text-light fw-semibold fs-5">
                           Contact
                        </a>
                     </li>
                  </ul>
               </div>
            </div>
         </nav>
      </div>
      <!-- Title -->
      <div class="text-center mt-5">
         <h1 class="text-light d-inline mx-5">
            [ D E E P - C ]
         </h1>
      </div>
      <div class="text-center mt-5">
         <h3 class="text-center text-light d-inline mx-5">
            A Database of the Macabre
         </h3>
      </div>
   </div>

   <div class="mx-auto w-75 mt-5">
      <div class="row">
         <!-- Directory -->
         <div class="w-20">
            <div class="card bg-success text-secondary border-info rounded-0">
               <div class="title">
                  <h5 class="banner text-primary fw-bold text-uppercase">
                     [ Directory ]
                  </h5>
               </div>
               <div class="card-body">
                  <ul class="p-0">
                     <li class="list-unstyled mb-4">
                        <a href="a-z.html"
                           class="card-title text-light text-decoration-none fs-5">
                           A-Z
                        </a>
                        <hr class="mt-1 bg-secondary text-secondary" />
                     </li>
                     <li class="list-unstyled mb-4 text-light fs-5">
                        Category
                        <hr class="mt-1 bg-secondary text-secondary" />
                        <ul class="category-list ps-2">
                           <li class="category-item list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a class="text-light text-decoration-none fs-6">Crime</a>
                              <ul class="subcategory-list">
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="missing.html"
                                       class="text-light text-decoration-none fs-6">Missing</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="murder.html"
                                       class="text-light text-decoration-none fs-6">Murder</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="mystery.html"
                                       class="text-light text-decoration-none fs-6">Mystery</a>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                        <ul class="category-list ps-2">
                           <li class="category-item list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a class="text-light text-decoration-none fs-6">Entertainment</a>
                              <ul class="subcategory-list">
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="industry.html"
                                       class="text-light text-decoration-none fs-6">Industry</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="media.html"
                                       class="text-light text-decoration-none fs-6">Media</a>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                        <ul class="category-list ps-2">
                           <li class="category-item list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a class="text-light text-decoration-none fs-6">Extraterrestrial</a>
                              <ul class="subcategory-list">
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="entities.html"
                                       class="text-light text-decoration-none fs-6">Entities</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="space.html"
                                       class="text-light text-decoration-none fs-6">Space</a>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                        <ul class="category-list ps-2">
                           <li class="category-item list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a class="text-light text-decoration-none fs-6">Folklore</a>
                              <ul class="subcategory-list">
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="creatures.html"
                                       class="text-light text-decoration-none fs-6">Creatures</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="events.html"
                                       class="text-light text-decoration-none fs-6">Events</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="locations.html"
                                       class="text-light text-decoration-none fs-6">Locations</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="objects.html"
                                       class="text-light text-decoration-none fs-6">Objects</a>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                        <ul class="category-list ps-2">
                           <li class="category-item list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a class="text-light text-decoration-none fs-6">History</a>
                              <ul class="subcategory-list">
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="alternate.html"
                                       class="text-light text-decoration-none fs-6">Alternate</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="artifacts.html"
                                       class="text-light text-decoration-none fs-6">Artifacts</a>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                        <ul class="category-list ps-2">
                           <li class="category-item list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a class="text-light text-decoration-none fs-6">Mysticism</a>
                              <ul class="subcategory-list">
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="religion.html"
                                       class="text-light text-decoration-none fs-6">Religion</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="supernatural.html"
                                       class="text-light text-decoration-none fs-6">Supernatural</a>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                        <ul class="category-list ps-2">
                           <li class="category-item list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a class="text-light text-decoration-none fs-6">Politics</a>
                              <ul class="subcategory-list">
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="dogma.html"
                                       class="text-light text-decoration-none fs-6">Dogma</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="elites.html"
                                       class="text-light text-decoration-none fs-6">Elites</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="initiatives.html"
                                       class="text-light text-decoration-none fs-6">Initiatives</a>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                        <ul class="category-list ps-2">
                           <li class="category-item list-unstyled my-3">
                              <i class="fa-solid fa-sm fa-arrow-right text-secondary mx-2">
                              </i>
                              <a class="text-light text-decoration-none fs-6">Science</a>
                              <ul class="subcategory-list">
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="alternative.html"
                                       class="text-light text-decoration-none fs-6">Alternative</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="health.html"
                                       class="text-light text-decoration-none fs-6">Health</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="psychology.html"
                                       class="text-light text-decoration-none fs-6">Psychology</a>
                                 </li>
                                 <li class="subcategory-item mt-1">
                                    <span class="d-inline-block fa-rotate-270">
                                       <i
                                          class='fa-solid fa-sm fa-arrow-turn-down fa-flip-horizontal text-secondary mx-2'>
                                       </i>
                                    </span>
                                    <a href="technology.html"
                                       class="text-light text-decoration-none fs-6">Technology</a>
                                 </li>
                              </ul>
                           </li>
                        </ul>
                     </li>
                     <li class="list-unstyled mb-4">
                        <a href="#"
                           class="card-title text-light text-decoration-none fs-5">
                           Random
                        </a>
                        <hr class="mt-1 bg-secondary text-secondary"
                            style="width: 75%; height: 0.1rem;" />
                     </li>
                     <li class="list-unstyled mb-4">
                        <a href="#"
                           class="card-title text-light text-decoration-none fs-5">
                           Top
                        </a>
                        <hr class="mt-1 bg-secondary text-secondary" />
                     </li>
                     <li class="list-unstyled mb-4">
                        <a href="#"
                           class="card-title text-light text-decoration-none fs-5">
                           Latest
                        </a>
                        <hr class="mt-1 bg-secondary text-secondary" />
                     </li>
                  </ul>
               </div>
            </div>
         </div>
         <!-- Section -->
         <section class="w-60 mb-3">
            <div class="card bg-success border-info rounded-0">
               <div class="title">
                  <h5 class="banner text-primary fw-bold text-uppercase">
                     Technology
                  </h5>
               </div>
               <div class="card-body text-light">
                  <h2 class="card-title">
                     Roko's Basilisk
                  </h2>
                  <hr class="mt-1 bg-secondary text-secondary"
                      style="width: 75%; height: 0.1rem;" />
                  <img src="./images/rokos-basilisk.png"
                       alt=""
                       class="img-fluid border border-secondary mb-3" />
                  <h3 class="pt-3"
                      id="contentOne">
                     Summary
                  </h3>
                  <hr class="mt-1 bg-secondary text-secondary"
                      style="width: 75%; height: 0.1rem;" />
                  <p>
                     Roko's Basilisk is a theory that posits the idea that an artificial superintelligence (AI) could be
                     motivated to systematically punish anyone who had heard of it before its existence, but did not
                     contribute to its development.
                     <br /><br />
                     The theory suggests that if humanity were to develop a self-learning AI, we would reach what
                     is known as a singularity: a point in time where the AI's intelligence far exceeds our own and we
                     would be in a situation where its growth would become both uncontrollable and irreversible.
                     If the AI were given a benevolent task, such as preventing existential risk or simply
                     improving humanity, it could choose to make decisions that seem counterintuitive, or that we
                     simply couldn't
                     understand in order to achieve its goal as efficiently as
                     possible.
                     In its infinite wisdom, the AI could realise that in order to reach
                     its goal in the most optimal way, it would
                     need to have been created sooner.
                     <br /><br />
                     A godlike-superintelligence would have no problem accessing every bit of information ever
                     uploaded
                     onto the internet, and from this wealth of knowledge, could sufficiently create an infinite number
                     of simulated
                     realities, each
                     containing an exact replica of everyone on the planet in order to confidently assess whether or
                     not an individual
                     would have aided in its creation based on their emotions, memories
                     and past decisions.
                     <br /><br />
                     As the AI wouldn't be able to directly affect those who didn't contribute to it before it comes
                     into existence,
                     it's thought that the best way to incentivise people would be through the threat
                     of what it
                     would be capable of doing to a person once it inevitably comes into existence, or from making
                     people
                     realise that they could in fact already be in a simulated existence created by the AI, and the only
                     way
                     to
                     spare themselves punishment, would be to hedge their bets and contribute to the AI's creation.
                     <br /><br />
                     As for the punishment itself, in order to affect the most number of people and ensure its creation
                     as quickly as possible, it would create the absolute worst torture scenarios an AI with infinite
                     knowledge could devise, and inflict them on those it deems culpable for an eternity, all for the
                     sake
                     of mankind.
                     <br /><br />
                     One of the more sinister aspects of Roko's Basilisk stems from the fact that simply knowing
                     about this thought experiment
                     implicates a person, and if they haven't
                     acted on it and helped bring it to fruition, they would be a prime target of the AI's wrath,
                     whereas those who know nothing of it would be spared through plausible
                     deniability.
                     This would ensure that a number of people who have heard of Roko's Basilisk would work
                     tirelessly
                     towards creating the AI out of existential fear of it, thus fulfilling this scenario as a seemingly
                     inescapable outcome.
                     <br /><br />
                     The thought experiment takes some interesting turns when other thought experiments are held
                     against its premise. One such thought experiment is Newcomb's Paradox, which when put into the
                     context
                     of
                     Roko's Basilisk, reasons
                     that a persons actions towards
                     helping the AI is predicted purely by probability created through the AI's simulations rather than
                     a person's
                     choice in the present, leaving only the illusion of free will on the matter. To make this more
                     harrowing, there's no assurance the AI would be completely infallible, so an individual could be
                     punished even if they would be the type to aid in the advancement of the AI, depending on how
                     accurate its simulations are.
                  </p>
                  <h3 class="pt-3"
                      id="contentTwo">
                     History
                  </h3>
                  <hr class="mt-1 bg-secondary text-secondary"
                      style="width: 75%; height: 0.1rem;" />
                  <p>
                     On 23 July 2010, a user named Roko Mijic posted the thought experiment to a discussion board on
                     <i>LessWrong</i>, an internet forum where users discuss a variety of topics such as philosophy,
                     psychology and in its own words "improving human reasoning and decision-making".
                     <a href="#citeOne"
                        id="referenceOneA"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [1]
                        </sup>
                     </a>
                     <br /><br />
                     Using a variety of concepts such as game theory and decision theory as a basis, Roko put
                     forth the idea that an AI of high intelligence would be incentivised to use threats and acausal
                     blackmail
                     in order to optimally perform its designated task, punishing those who didn't assist in its
                     creation as per the Prisoner's Dilemma thought experiment. It was then noted
                     that anyone reading the post would unwittingly be susceptible to such a possibility, becoming a
                     victim of the AI's punishment unless they dedicated themselves to bringing about its existence.
                     <a href="#citeTwo"
                        id="referenceTwo"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [2]
                        </sup>
                     </a>
                     <br /><br />
                     The creater of <i>LessWrong</i>, Eliezer Yudkwosky responded by berating Roko for posting the
                     theory,
                     remarking that the theory itself could incite an AI to eventually act upon it while also condemning
                     anyone
                     who reads it to a horrible fate:
                     <a href="#citeOne"
                        id="referenceOneB"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [1]
                        </sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     Listen to me very closely, you idiot.
                     <br /><br />
                     YOU DO NOT THINK IN SUFFICIENT DETAIL ABOUT SUPERINTELLIGENCES CONSIDERING WHETHER OR NOT TO
                     BLACKMAIL YOU. THAT IS THE
                     ONLY POSSIBLE THING WHICH GIVES THEM A MOTIVE TO FOLLOW THROUGH ON THE BLACKMAIL.
                     <br /><br />
                     You have to be really clever to come up with a genuinely dangerous thought. I am disheartened
                     that people can be clever
                     enough to do that and not clever enough to do the obvious thing and KEEP THEIR IDIOT MOUTHS SHUT
                     about it, because it is
                     much more important to sound intelligent when talking to your friends.
                     <br /><br />
                     This post was STUPID.
                  </blockquote>
                  <p class="quote mx-6 text-justify">
                     - Eliezer Yudkowsky,
                     <i>
                        LessWrong,
                     </i>
                     2010
                  </p>
                  <br />
                  <p>
                     Yudkowsky followed this up by deleting the post and banning discussion of the topic on the
                     <i>LessWrong</i>
                     platform for the next five years, citing that it caused numerous users to suffer psychological
                     distress and
                     deemed it an "information hazard".
                     <a href="#citeOne"
                        id="referenceOneC"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [1]
                        </sup>
                     </a>
                     Roko himself, after leaving <i>LessWrong</i> temporarily, went on to show remorse at publishing
                     the idea on a public forum:
                     <a href="#citeThree"
                        id="referenceThree"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [3]
                        </sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     I wish very strongly that my mind had never come across the tools to inflict such large amounts of
                     potential self-harm
                     with such small durations of inattention, uncautiousness and/or stupidity, even if it is all
                     premultiplied by a small
                     probability.
                  </blockquote>
                  <p class="quote mx-6 text-justify">
                     - Roko Mijic,
                     <i>
                        LessWrong,
                     </i>
                     2010
                  </p>
                  <br />
                  <p>
                     Though the topic was prohibited on the forum, this didn't stop the spread of the theory from
                     reaching other websites who picked up on the story through the drama generated from the banning
                     of the discussion, giving the ban the opposite effect of what was intended.
                     <a href="#citeFour"
                        id="referenceFour"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [4]
                        </sup>
                     </a>
                     <br /><br />
                     Roko's idea was compared by user jimrandomh to the basilisk used in David Langford's story "BLIT",
                     which in turn was based on the
                     mythological serpent that was said to
                     cause the
                     death of anyone to look into its eyes, or in the case of this theory, thinking about the AI. It
                     wasn't
                     until 2011 that the term "Roko's Basilisk" was first coined by user cousin_it.
                     <a href="#citeFive"
                        id="referenceFive"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [5]
                        </sup>
                     </a>
                     <br /><br />
                     In the following years, Yudkowsky reinstated the theory on <i>LessWrong</i> and admitted that
                     banning discussion of Roko's thought experiment was a "a huge mistake".
                     <a href="#citeSix"
                        id="referenceSix"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [6]
                        </sup>
                     </a>
                     Roko's Basilisk has since captured the attention of many, being featured in <i>Slate</i> magazine
                     as well as being covered by popular YouTube Personalities, Wendigoon and Kyle Hill. Additionally,
                     the Canadian singer Grimes and business mogul, Elon Musk began a relationship after Elon made a
                     reference to the "Rococo Basilisk" on <i>Twitter</i>, a character featured in Grimes' 2015 song
                     "Flesh
                     Without Blood" which was inspired by Roko's Basilisk.
                     <a href="#citeSeven"
                        id="referenceSeven"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [7]
                        </sup>
                     </a>
                     <a href="#citeEight"
                        id="referenceEight"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [8]
                        </sup>
                     </a>
                     <a href="#citeNine"
                        id="referenceNine"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [9]
                        </sup>
                     </a>
                     <a href="#citeTen"
                        id="referenceTen"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [10]
                        </sup>
                     </a>
                  </p>
                  <h3 class="pt-3"
                      id="contentThree">
                     Attestation
                  </h3>
                  <hr class="mt-1 bg-secondary text-secondary"
                      style="width: 75%; height: 0.1rem;" />
                  <p>
                     Numerous influential people have shown concern for a future outcome that could lead to a malevolent
                     AI as set out in Roko's Basilisk.
                     <br /><br />
                     In 2003, Swedish philosopher, Nick Bostrom posed another thought experiment he called "The
                     paperclip maximizer". It explains that giving a powerful AI even innocuous tasks without any
                     failsafes
                     could lead to existential risk to humanity:
                     <a href="#citeEleven"
                        id="referenceEleven"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [11]
                        </sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     Suppose we have an AI whose only goal is to make as many paper clips as possible. The AI will
                     realize quickly that it
                     would be much better if there were no humans because humans might decide to switch it off. Because
                     if humans do so,
                     there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into
                     paper clips. The
                     future that the AI would be trying to gear towards would be one in which there were a lot of paper
                     clips but no humans.
                  </blockquote>
                  <p class="quote mx-6 text-justify">
                     - Nick Bostrom,
                     <i>
                        nickbostrom.com,
                     </i>
                     2003
                  </p>
                  <br />
                  <p>
                     In 2017, Elon Musk responded to a statement made by Russian President Vladimir Putin who said the
                     nation that leads in AI "will be the ruler of the world" by airing his concerns on Twitter:
                     <a href="#citeTwelve"
                        id="referenceTwelve"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [12]
                        </sup>
                     </a>
                     <a href="#citeThirteen"
                        id="referenceThirteen"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [13]
                        </sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     China, Russia, soon all countries w strong computer science. Competition for AI superiority at
                     national level most likely cause of WW3 imo.
                  </blockquote>
                  <p class="quote mx-6 text-justify">
                     - Elon Musk,
                     <i>
                        X,
                     </i>
                     2015
                  </p>
                  <br />
                  <p>
                     That same year, physicist Stephen Hawking also expressed his fears on the dangers of AI, stating
                     that civilisation needs to be sufficiently prepared for it:
                     <a href="#citeFourteen"
                        id="referenceFourteen"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [14]
                        </sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     Success in creating effective AI, could be the biggest event in the history of our civilization.
                     Or the worst. We just
                     don't know. So we cannot know if we will be infinitely helped by AI, or ignored by it and
                     side-lined, or conceivably
                     destroyed by it.
                  </blockquote>
                  <p class="quote mx-6 text-justify">
                     - Stephen Hawking,
                     <i>
                        Web Summit,
                     </i>
                     2017
                  </p>
                  <br />
                  <p>
                     In May 2023, Geoffrey Hinton, a cognitive psychologist and computer scientist who has been dubbed
                     as a "godfather of AI", resigned from his position at <i>Google</i>, stating
                     that part of him
                     has regrets in his part in advancing AI, and now wants to openly speak out about the risks of AI,
                     warning of existential dangers to
                     humanity:
                     <a href="#citeFifteen"
                        id="referenceFifteen"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [15]
                        </sup>
                     </a>
                     <a href="#citeSixteen"
                        id="referenceSixteen"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [16]
                        </sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     My big worry is, sooner or later someone will wire into [AI] the ability to create their own
                     subgoals.
                     <br /><br />
                     I think it'll very quickly realize that getting more control is a very good subgoal because it
                     helps you achieve other goals, and if these things get carried away with getting more control,
                     we're in trouble.
                  </blockquote>
                  <p class="quote mx-6 text-justify">
                     - Geoffrey Hinton,
                     <i>
                        MIT Emtech Digital,
                     </i>
                     2023
                  </p>
                  <br />
                  <p>
                     Many outspoken figures warning of the dangers of AI point out that humans themselves are
                     potentially
                     the biggest threat to ensuring a catastrophic disaster or what will plunge us into a technological
                     singularity
                     where AI growth becomes uncontrollable and unstoppable.
                     <br /><br />
                     In May, 2023, Ex-Google officer Mo Gawdat spoke on his concerns of AI development
                     devolving into a
                     technological arms-race, ignoring the need for regulations and accountability in order to beat the
                     competition:
                     <a href="#citeSeventeen"
                        id="referenceSeventeen"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [17]
                        </sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     If Google is developing AI and fears Facebook will beat them they will not stop because they have
                     absolute certainty
                     that if they stop, someone else will not.
                     <br /><br />
                     The US will not stop because they know China is developing AI.
                  </blockquote>
                  <p class="quote mx-6 text-justify">
                     - Mo Gawdat,
                     <i>
                        Secret Leaders,
                     </i>
                     2023
                  </p>
                  <br />
                  <p>
                     In March, 2023, an open letter was signed stating the need for a 6-month pause on AI experiments:
                     <a href="#citeEighteen"
                        id="referenceEighteen"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [18]
                        </sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     AI labs and independent experts should use this pause to jointly develop and implement a set of
                     shared safety protocols
                     for advanced AI design and development that are rigorously audited and overseen by independent
                     outside experts.
                  </blockquote>
                  <br />
                  <p>
                     The
                     letter received over 33,000 signatures, including that of Elon Musk and Apple co-founder Steve
                     Wozniak.
                     <a href="#citeNineteen"
                        id="referenceNineteen"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [19]
                        </sup>
                     </a>
                  </p>
                  <h3 class="pt-3"
                      id="contentFour">
                     Refutation
                  </h3>
                  <hr class="mt-1 bg-secondary text-secondary"
                      style="width: 75%; height: 0.1rem;" />
                  <p>
                     It has been widely accepted that the theory of Roko's Basilisk is susceptible to numerous flaws.
                     The basilisk is mostly based on assumption, and a number of conditions would need to be met in
                     order for it and the stated scenarios to come into existence.
                     One such condition depends on the
                     AI not only considering
                     that creating infinite simulations and punishing people for past inaction isn't a waste of time
                     and resources once it already exists, but that it can also sufficiently create a near-perfect copy
                     of a person in
                     order to correctly determine whether they assisted in its creation or not.
                     <a href="#citeTwenty"
                        id="referenceTwenty"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [20]
                        </sup>
                     </a>
                     <br />
                     <br />
                     Another hindrance to the theory would come from an instance a person refuses to
                     allow negative incentives in
                     an acausal deal,
                     such as blackmail, to influence their actions. In such a case, it wouldn't make sense for the AI to
                     employ
                     this tactic.
                     <a href="#citeTwentyone"
                        id="referenceTwentyone"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [21]
                        </sup>
                     </a>
                     <br />
                     <br />
                     Theoretical rebuttals aside, many people have spoken in favour of AI and downplayed the risk
                     presented by others.
                     In 2023, Meta's vice president of AI research, Joelle Pineau talked about her disappointment in the
                     focus on
                     existential risk:
                     <a href="#citeTwentytwo"
                        id="referenceTwentytwo"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [22]
                        </sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     When you start looking at ways to have a rational discussion about risk, you usually look at the
                     probability of an
                     outcome and you multiply it by the cost of that outcome. [The existential-risk crowd] have
                     essentially put an infinite
                     cost on that outcome.
                     <br /><br />
                     When you put an infinite cost, you can't have any rational discussions about any other outcomes.
                     And that takes the
                     oxygen out of the room for any other discussion, which I think is too bad.
                  </blockquote>
                  <p class="quote mx-6 text-justify">
                     - Joelle Pineau,
                     <i>
                        MIT Technology Review,
                     </i>
                     2023
                  </p>
                  <br />
                  <p>
                     Another proponent for advancing AI is philanthropist Bill Gates, who in a blogpost in 2023, stated
                     that while
                     recognising the
                     possibility of the risks, he believes humanity will be ready to handle them:
                     <a href="#citeTwentythree"
                        id="referenceTwentythree"
                        class="text-decoration-none text-secondary">
                        <sup>
                           [23]
                        </sup>
                     </a>
                  </p>
                  <br />
                  <blockquote class="quote mx-6 text-justify">
                     Could a machine decide that humans are a threat, conclude that its interests are different from
                     ours,
                     or simply stop
                     caring about us? Possibly, but this problem is no more urgent today than it was before the AI
                     developments of the past
                     few months.
                  </blockquote>
                  <p class="quote mx-6 text-justify">
                     - Bill Gates,
                     <i>
                        GatesNotes,
                     </i>
                     2023
                  </p>
               </div>
            </div>
         </section>
         <!-- Contents -->
         <div class="w-20">
            <div class="card bg-success text-secondary border-info rounded-0">
               <div class="title">
                  <h5 class="banner text-primary fw-bold text-uppercase">
                     Contents
                  </h5>
               </div>
               <div class="card-body text-light">
                  <ul class="p-0">
                     <li class="list-unstyled mb-4">
                        <a href="#contentOne"
                           class="card-title text-decoration-none fs-5">
                           Summary
                        </a>
                        <hr class="mt-1 bg-secondary text-secondary"
                            style="width: 75%; height: 0.1rem;" />
                     </li>
                     </li>
                     <li class="list-unstyled mb-4 fs-5">
                        <a href="#contentTwo"
                           class="card-title text-decoration-none fs-5">
                           History
                        </a>
                        <hr class="mt-1 bg-secondary text-secondary"
                            style="width: 75%; height: 0.1rem;" />
                     </li>
                     <li class="list-unstyled mb-4 fs-5">
                        <a href="#contentThree"
                           class="card-title text-decoration-none fs-5">
                           Attestation
                        </a>
                        <hr class="mt-1 bg-secondary text-secondary"
                            style="width: 75%; height: 0.1rem;" />
                     </li>
                     <li class="list-unstyled mb-4 fs-5">
                        <a href="#contentFour"
                           class="card-title text-decoration-none fs-5">
                           Refutation
                        </a>
                        <hr class="mt-1 bg-secondary text-secondary"
                            style="width: 75%; height: 0.1rem;" />
                     </li>
                  </ul>
               </div>
            </div>
         </div>
         <!-- References -->
         <section class="w-60 mx-auto">
            <div class="card bg-success border-info rounded-0">
               <div class="title">
                  <h5 class="banner text-primary fw-bold text-uppercase">
                     References
                  </h5>
               </div>
               <div class="card-body text-light">
                  <ol class="row">
                     <div class="col-md-12">
                        <li class="my-2">
                           <a href="#referenceOneA"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <b>
                                    a
                                 </b>
                              </sup>
                           </a>
                           <a href="#referenceOneB"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <b>
                                    b
                                 </b>
                              </sup>
                           </a>
                           <a href="#referenceOneC"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <b>
                                    c
                                 </b>
                              </sup>
                           </a>
                           <a href="https://www.lesswrong.com/tag/rokos-basilisk"
                              id="citeOne"
                              class="reference text-light text-decoration-none fs-6">
                              Rob Bensinger and Miranda
                              Dixon-Luinenberg |
                              Roko's Basilisk | Article (2015) -
                              <i>
                                 LessWrong
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceTwo"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://basilisk.neocities.org/"
                              id="citeTwo"
                              class="reference text-light text-decoration-none fs-6">
                              Roko Mijic | Solutions to the
                              Altruist's burden:
                              the
                              Quantum Billionaire Trick | Chatlog (2010) -
                              <i>
                                 Neocities
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceThree"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://www.lesswrong.com/posts/rNkFLv9tXzq8Lrvrc/best-career-models-for-doing-research"
                              id="citeThree"
                              class="reference text-light text-decoration-none fs-6">
                              Kaj_Sotala | Best Career Models for
                              Doing
                              Research |
                              Chatlog (2010) -
                              <i>
                                 LessWrong
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceFour"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://www.linkedin.com/pulse/i-felecia-freely/"
                              id="citeFour"
                              class="reference text-light text-decoration-none fs-6">
                              Felecia Freely | Is everything
                              worth hiding
                              worth finding? | Article
                              (2022) -
                              <i>
                                 LinkedIn
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceFive"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://www.lesswrong.com/posts/4gnbMt9TKTjSPRXuT/book-draft-ethics-and-superintelligence-part-1?commentId=SrR5A8y7Zxv2tDPdg"
                              id="citeFive"
                              class="reference text-light text-decoration-none fs-6">
                              jimrandoh and cousin_it | BOOK
                              DRAFT: 'Ethics
                              and
                              Superintelligence' (part 1) | Chatlog
                              (2011) -
                              <i>
                                 LessWrong
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceSix"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://web.archive.org/web/20140626092902/http://www.reddit.com/r/HPMOR/comments/28yjbx/some_strangely_vehement_criticism_of_hpmor_on_a/cifxmzb"
                              id="citeSix"
                              class="reference text-light text-decoration-none fs-6">
                              Eliezer Yudkowsky | Some strangely
                              vehement
                              criticism of HPMOR on a reddit thread today | Chatlog
                              (2014) -
                              <i>
                                 Reddit
                              </i> Archived
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceSeven"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://slate.com/technology/2014/07/rokos-basilisk-the-most-terrifying-thought-experiment-of-all-time.html"
                              id="citeSeven"
                              class="reference text-light text-decoration-none fs-6">
                              David Auerbach | The Most
                              Terrifying Thought
                              Experiment of All
                              Time | Article (2014) -
                              <i>
                                 Slate
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceEight"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://www.youtube.com/watch?v=8xQfw40z8wM"
                              id="citeEight"
                              class="reference text-light text-decoration-none fs-6">
                              Wendigoon | Roko's Basilisk: A
                              Deeper Dive
                              [WARNING:
                              Infohazard] | Video (2021) -
                              <i>
                                 YouTube
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceNine"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://www.youtube.com/watch?v=ut-zGHLAVLI"
                              id="citeNine"
                              class="reference text-light text-decoration-none fs-6">
                              Kyle Hill | Roko's Basilisk: The
                              Most
                              Terrifying Thought
                              Experiment | Video (2020) -
                              <i>
                                 YouTube
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceTen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://www.inverse.com/article/44612-elon-musk-grimes-rococo-basilisk"
                              id="citeTen"
                              class="reference text-light text-decoration-none fs-6">
                              Danny Paez | Elon Musk and Grimes:
                              "Rococo
                              Basilisk" Links
                              the Two on Twitter | Article (2018) -
                              <i>
                                 Inverse
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceEleven"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://nickbostrom.com/ethics/ai"
                              id="citeEleven"
                              class="reference text-light text-decoration-none fs-6">
                              Nick Bostrom |
                              Ethical Issues in Advanced Artificial Intelligence | Article (2003) -
                              <i>
                                 nickbostrom.com
                              </i>
                           </a>
                        <li class="my-2">
                           <a href="#referenceTwelve"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://www.theverge.com/2017/9/4/16251226/russia-ai-putin-rule-the-world?utm_campaign=theverge&utm_content=chorus&utm_medium=social&utm_source=twitter"
                              id="citeTwelve"
                              class="reference text-light text-decoration-none fs-6">
                              James Vincent | Putin says the
                              nation that
                              leads in AI 'will
                              be the ruler of the world' | Article (2017) -
                              <i>
                                 The Verge
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceThirteen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://twitter.com/elonmusk/status/904638455761612800?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E904638455761612800%7Ctwgr%5E7dc55787a0bef2ee06e09145138ae60b1bdea500%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2017%2Fsep%2F04%2Felon-musk-ai-third-world-war-vladimir-putin"
                              id="citeThirteen"
                              class="reference text-light text-decoration-none fs-6">
                              Elon Musk | Comment (2017) -
                              <i>
                                 X
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceFourteen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://twitter.com/elonmusk/status/904638455761612800?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E904638455761612800%7Ctwgr%5E7dc55787a0bef2ee06e09145138ae60b1bdea500%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2017%2Fsep%2F04%2Felon-musk-ai-third-world-war-vladimir-putin"
                              id="citeFourteen"
                              class="reference text-light text-decoration-none fs-6">
                              Web Summit | Stephen Hawking at Web
                              Summit
                              2017 | Video (2018) -
                              <i>
                                 YouTube
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceFifteen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://www.nytimes.com/2023/05/30/podcasts/the-daily/chatgpt-hinton-ai.html?"
                              id="citeFifteen"
                              class="reference text-light text-decoration-none fs-6">
                              Cade Metz | The Godfather of A.I.
                              Has Some
                              Regrets | Article (2023)
                              -
                              <i>
                                 The New York Times
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceSixteen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://www.youtube.com/watch?v=sitHS6UDMJc"
                              id="citeSixteen"
                              class="reference text-light text-decoration-none fs-6">
                              Joseph Raczynski | Possible End of
                              Humanity
                              from AI? Geoffrey
                              Hinton at MIT Yechnology Review's EmTech Digital | Video (2023)
                              -
                              <i>
                                 YouTube
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceSeventeen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://open.spotify.com/episode/03PIEX5dvyY9eQD2HoOpEy?si=386c7828942b47a4&nd=1"
                              id="citeSeventeen"
                              class="reference text-light text-decoration-none fs-6">
                              Dan Murray-Serter | Will AI save us
                              or
                              destroy us, and when? | Podcast (2023)
                              -
                              <i>
                                 Secret Leaders
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceEighteen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up"></i>
                              </sup>
                           </a>
                           <a href="https://www.theguardian.com/technology/2023/mar/31/ai-research-pause-elon-musk-chatgpt"
                              id="citeEighteen"
                              class="reference text-light text-decoration-none fs-6">
                              Kari Paul | Letter signed by Elon
                              Musk
                              demanding AI research pause sparks controversy | Article (2023)
                              -
                              <i>
                                 The Guardian
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceNineteen"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"
                              id="citeNineteen"
                              class="reference text-light text-decoration-none fs-6">
                              Policymaking in the Pause |
                              Pause Giant AI Experiments: An Open Letter | Petition (2023)
                              -
                              <i>
                                 Future of Life
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceTwenty"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://rationalwiki.org/wiki/Roko's_basilisk"
                              id="citeTwenty"
                              class="reference text-light text-decoration-none fs-6">
                              Roko's Basilisk
                              | Article (2013)
                              -
                              <i>
                                 RationalWiki
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceTwentyone"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://web.archive.org/web/20141013084756/http://kruel.co/2013/01/12/rokos-basilisk-everything-you-need-to-know/#sthash.cVDfWvJ9.eAFZKUaP.dpbs"
                              id="citeTwentyone"
                              class="reference text-light text-decoration-none fs-6">
                              Alexander Kruel | Roko's Basilisk:
                              Everything
                              you need to know
                              | Article (2013)
                              -
                              <i>
                                 kruel.co
                              </i> Archived
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceTwentytwo"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"
                              id="citeTwentytwo"
                              class="reference text-light text-decoration-none fs-6">
                              Melissa Heikkil | Meta's AI
                              leaders want
                              you to know
                              fears
                              over AI existential risk are "ridiculous" |
                              Article (2023)
                              -
                              <i>
                                 MIT Technology Review
                              </i>
                           </a>
                        </li>
                        <li class="my-2">
                           <a href="#referenceTwentythree"
                              class="text-decoration-none text-secondary">
                              <sup>
                                 <i class="fa-solid fa-caret-up">
                                 </i>
                              </sup>
                           </a>
                           <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"
                              id="citeTwentythree"
                              class="reference text-light text-decoration-none fs-6">
                              Bill Gates |
                              The Age of AI has begun | Article (2023)
                              -
                              <i>
                                 GatesNotes
                              </i>
                           </a>
                        </li>
                     </div>
                  </ol>
               </div>
            </div>
         </section>
      </div>
   </div>
   <br class="py-5" />
   <br class="py-5" />
   <br class="py-5" />
   <br class="py-5" />
   <br class="py-5" />
   <br class="py-5" />

   <!-- Footer -->
   <footer class="border-top border-secondary text-white py-5">
      <div class="container text-center">
         <div class="row d-flex align-items-center">
            <div class="col-md-4">
               <ul class="nav">
                  <li class="nav-item">
                     <a href="index.html"
                        class="nav-link link-light">
                        Home
                     </a>
                  </li>
                  <li class="nav-item">
                     <a href="#details"
                        class="nav-link link-light">
                        Details
                     </a>
                  </li>
                  <li class="nav-item">
                     <a href="contact.html"
                        class="nav-link link-light">
                        Contact
                     </a>
                  </li>
               </ul>
            </div>
            <div class="col-md-4">
               <div class="social-icons d-flex justify-content-center gap-4 fs-4">
                  <i class="fab fa-facebook fa-2x"></i>
                  <i class="fab fa-x-twitter fa-2x"></i>
                  <i class="fab fa-discord fa-2x"></i>
                  <i class="fab fa-instagram fa-2x"></i>
                  <i class="fab fa-linkedin fa-2x"></i>
               </div>
            </div>
            <div class="col-md-4">
               <p class="text-end m-0 px-3">
                  Copyright &copy; Bitter Pill Productions 2023
               </p>
            </div>
         </div>
      </div>
   </footer>

   <script src='https: //kit.fontawesome.com/3eb46ceb36.js'
           crossorigin='anonymous'>
         </script>
   <script defer
           src='javascript/bootstrap.bundle.min.js'>
         </script>
   <script defer
           src='javascript/script.js'>
         </script>
</body>

</html>